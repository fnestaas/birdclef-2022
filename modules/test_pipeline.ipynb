{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Pipeline\n",
    "In this notebook we just test the implemented modules to verify that everything works.\n",
    "\n",
    "## Notes\n",
    "- If we want to apply mixup, we have to do so before passing the data to the model. In their implementation, when the model is in training mode, it gets both x and y and computes the mixup during the forward call. The way we have set this up here, the model expects *only* x, so if we want Mixup, we have to do so on the batch _before_ passing it to the model.\n",
    "- The modules themselves are still very simple and can be improved/we can add more functionality to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "from TransformApplier import TransformApplier \n",
    "from Wav2Spec import Wav2Spec\n",
    "from SimpleDataset import SimpleDataset\n",
    "import pandas as pd \n",
    "from PretrainedModel import *\n",
    "from OnlyXTransform import OnlyXTransform\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../birdclef-2022/'\n",
    "metadata = pd.read_csv(f'{DATA_PATH}train_metadata.csv')\n",
    "\n",
    "with open(f'{DATA_PATH}scored_birds.json') as f:\n",
    "    birds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleDataset(metadata, DATA_PATH, labels=birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_audio/akepa1/XC122473.ogg\n",
      "22 (tensor([ 4.8033e-06,  2.8010e-06,  5.4884e-06,  ..., -2.1241e-02,\n",
      "        -1.3902e-02, -4.4505e-03]), array([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(1000):\n",
    "    d = dataset.__getitem__(i, debug=False)\n",
    "    if np.sum(d[-1]) > 2:\n",
    "        dataset.__getitem__(i, debug=True)\n",
    "        print(i, d)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import Mixup # has to be applied BEFORE THE MODEL SEES THE DATA! \n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Example post-processing step\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, width=512, n_out=1):\n",
    "        super().__init__()\n",
    "        self.att = nn.Sequential(nn.Linear(n_in, width), nn.ReLU(), nn.Linear(width, n_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight = torch.softmax(self.att(x), axis=1)\n",
    "        return (x * weight).sum(1)\n",
    "\n",
    "transforms1 = TransformApplier([nn.Identity()])\n",
    "\n",
    "wav2spec = Wav2Spec()\n",
    "\n",
    "transforms2 = TransformApplier([OnlyXTransform()])\n",
    "\n",
    "cnn = PretrainedModel(\n",
    "    model_name='efficientnet_b2', \n",
    "    in_chans=1, # normally 3 for RGB-images\n",
    ")\n",
    "\n",
    "transforms3 = TransformApplier([SimpleAttention(cnn.get_out_dim())])\n",
    "\n",
    "output_head = OutputHead(n_in=cnn.get_out_dim(), n_out=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = nn.Sequential(\n",
    "    transforms1, \n",
    "    wav2spec,\n",
    "    transforms2, \n",
    "    cnn,\n",
    "    transforms3, \n",
    "    output_head,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(pipeline(dataset[2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad735b6331381b4aa24eee4bc884a68ab8bc065c6ee95bf0f72270a9ca226ae5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
